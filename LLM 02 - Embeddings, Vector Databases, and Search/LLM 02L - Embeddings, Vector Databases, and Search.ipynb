{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Databricks notebook source\n",
        "# MAGIC %md-sandbox\n",
        "# MAGIC\n",
        "# MAGIC <div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
        "# MAGIC   <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
        "# MAGIC </div>\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# MAGIC %md\n",
        "# MAGIC\n",
        "# MAGIC # 02L - Embeddings, Vector Databases, and Search\n",
        "# MAGIC\n",
        "# MAGIC\n",
        "# MAGIC In this lab, we will apply the text vectorization, search, and question answering workflow that you learned in the demo. The dataset we will use this time will be on talk titles and sessions from [Data + AI Summit 2023](https://www.databricks.com/dataaisummit/). \n",
        "# MAGIC\n",
        "# MAGIC ### ![Dolly](https://files.training.databricks.com/images/llm/dolly_small.png) Learning Objectives\n",
        "# MAGIC 1. Learn how to use Chroma to store your embedding vectors and conduct similarity search\n",
        "# MAGIC 1. Use OpenAI GPT-3.5 to generate response to your prompt\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# MAGIC %pip install chromadb==0.3.21 tiktoken==0.3.3\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# MAGIC %md\n",
        "# MAGIC\n",
        "# MAGIC ## Classroom Setup\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# MAGIC %run ../Includes/Classroom-Setup\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# MAGIC %md\n",
        "# MAGIC ## Read data\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "dais_pdf = pd.read_parquet(f\"{DA.paths.datasets}/dais/dais23_talks.parquet\")\n",
        "display(dais_pdf)\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "dais_pdf[\"full_text\"] = dais_pdf.apply(\n",
        "    lambda row: f\"\"\"Title: {row[\"Title\"]}\n",
        "                Abstract:  {row[\"Abstract\"]}\"\"\".strip(),\n",
        "    axis=1,\n",
        ")\n",
        "print(dais_pdf.iloc[0][\"full_text\"])\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "texts = []\n",
        "for i, row in dais_pdf.iterrows():\n",
        "    texts.append(row[\"full_text\"])\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# MAGIC %md\n",
        "# MAGIC ## Question 1\n",
        "# MAGIC Set up Chroma and create collection\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "\n",
        "chroma_client = chromadb.Client(\n",
        "    Settings(\n",
        "        chroma_db_impl=\"duckdb+parquet\",\n",
        "        persist_directory=DA.paths.user_db,  # this is an optional argument. If you don't supply this, the data will be ephemeral\n",
        "    )\n",
        ")\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# MAGIC %md\n",
        "# MAGIC\n",
        "# MAGIC Fill out `collection_name` below.\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# TODO\n",
        "collection_name = \"<FILL_IN>\"\n",
        "\n",
        "# If you have created the collection before, you need to delete the collection first\n",
        "if len(chroma_client.list_collections()) > 0 and collection_name in [chroma_client.list_collections()[0].name]:\n",
        "    chroma_client.delete_collection(name=collection_name)\n",
        "else:\n",
        "    print(f\"Creating collection: '{collection_name}'\")\n",
        "    talks_collection = chroma_client.create_collection(name=collection_name)\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion2_1(collection_name)\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# MAGIC %md\n",
        "# MAGIC ## Question 2\n",
        "# MAGIC\n",
        "# MAGIC Add data to collection\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# TODO\n",
        "talks_collection.add(\n",
        "    documents=<FILL_IN>,\n",
        "    ids=<FILL_IN>\n",
        ")\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion2_2(talks_collection)\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# MAGIC %md\n",
        "# MAGIC ## Question 3\n",
        "# MAGIC\n",
        "# MAGIC Query for relevant documents\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# TODO\n",
        "import json\n",
        "\n",
        "results = talks_collection.query(\n",
        "    query_texts=<FILL_IN>,\n",
        "    n_results=<FILL_IN>\n",
        ")\n",
        "\n",
        "print(json.dumps(results, indent=4))\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion2_3(results)\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# MAGIC %md\n",
        "# MAGIC ## Question 4\n",
        "# MAGIC\n",
        "# MAGIC Load language model\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# TODO\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# Pick a model from HuggingFace that can generate text\n",
        "model_id = \"<FILL_IN>\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "lm_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"<FILL_IN>\", model=lm_model, tokenizer=tokenizer, max_new_tokens=512, device_map=\"auto\", handle_long_generation=\"hole\"\n",
        ")\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion2_4(pipe)\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# MAGIC %md\n",
        "# MAGIC ## Question 5\n",
        "# MAGIC\n",
        "# MAGIC Prompt engineering for question answering\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# TODO\n",
        "# Come up with a question that you need the LLM assistant to help you with\n",
        "# A sample question is \"Help me find sessions related to XYZ\"\n",
        "question = \"<FILL_IN>\"\n",
        "\n",
        "# Provide all returned similar documents from the cell above below\n",
        "context = <FILL_IN>\n",
        "\n",
        "# Feel free to be creative how you construct the prompt. You can use the demo notebook as a jumpstart reference.\n",
        "# You can also provide more requirements in the text how you want the answers to look like.\n",
        "# Example requirement: \"Recommend top-5 relevant sessions for me to attend.\"\n",
        "prompt_template = <FILL_IN>\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion2_5(question, context, prompt_template)\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# MAGIC %md\n",
        "# MAGIC ## Question 6 \n",
        "# MAGIC\n",
        "# MAGIC Submit query for language model to generate response.\n",
        "# MAGIC\n",
        "# MAGIC Hint: If you run into the error `index out of range in self`, make sure to check out this [documentation page](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TextGenerationPipeline.__call__.handle_long_generation).\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# TODO\n",
        "lm_response = pipe(<FILL_IN>)\n",
        "print(lm_response[0][\"generated_text\"])\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion2_6(lm_response)\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# MAGIC %md\n",
        "# MAGIC Notice that the output isn't exactly helpful. Head on to using OpenAI to try out GPT-3.5 instead! \n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# MAGIC %md\n",
        "# MAGIC ## OPTIONAL (Non-Graded): Use OpenAI models for Q/A\n",
        "# MAGIC\n",
        "# MAGIC For this section to work, you need to generate an Open AI key. \n",
        "# MAGIC\n",
        "# MAGIC Steps:\n",
        "# MAGIC 1. You need to [create an account](https://platform.openai.com/signup) on OpenAI. \n",
        "# MAGIC 2. Generate an OpenAI [API key here](https://platform.openai.com/account/api-keys). \n",
        "# MAGIC\n",
        "# MAGIC Note: OpenAI does not have a free option, but it gives you $5 as credit. Once you have exhausted your $5 credit, you will need to add your payment method. You will be [charged per token usage](https://openai.com/pricing). **IMPORTANT**: It's crucial that you keep your OpenAI API key to yourself. If others have access to your OpenAI key, they will be able to charge their usage to your account! \n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# TODO\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<FILL IN>\"\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "import openai\n",
        "\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# MAGIC %md\n",
        "# MAGIC If you would like to estimate how much it would cost to use OpenAI, you can use `tiktoken` library from OpenAI to get the number of tokens from your prompt.\n",
        "# MAGIC\n",
        "# MAGIC\n",
        "# MAGIC We will be using `gpt-3.5-turbo` since it's the most economical option at ($0.002/1k tokens), as of May 2023. GPT-4 charges $0.04/1k tokens. The following code block below is referenced from OpenAI's documentation on [\"Managing tokens\"](https://platform.openai.com/docs/guides/chat/managing-tokens).\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "price_token = 0.002\n",
        "encoder = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "cost_to_run = len(encoder.encode(prompt_template)) / 1000 * price_token\n",
        "print(f\"It would take roughly ${round(cost_to_run, 5)} to run this prompt\")\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# MAGIC %md\n",
        "# MAGIC We won't have to create a new vector database again. We can just send our `context` from above to OpenAI. We will use their chat completion API to interact with `GPT-3.5-turbo`. You can refer to their [documentation here](https://platform.openai.com/docs/guides/chat).\n",
        "# MAGIC\n",
        "# MAGIC Something interesting is that OpenAI models use the system message to help their assistant to be more accurate. From OpenAI's [docs](https://platform.openai.com/docs/guides/chat/introduction):\n",
        "# MAGIC\n",
        "# MAGIC > Future models will be trained to pay stronger attention to system messages. The system message helps set the behavior of the assistant.\n",
        "# MAGIC\n",
        "# MAGIC\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# TODO\n",
        "gpt35_response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": <FILL_IN>},\n",
        "    ],\n",
        "    temperature=0, # 0 makes outputs deterministic; The closer the value is to 1, the more random the outputs are for each time you re-run.\n",
        ")\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "print(gpt35_response.choices[0][\"message\"][\"content\"])\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "from IPython.display import Markdown\n",
        "\n",
        "Markdown(gpt35_response.choices[0][\"message\"][\"content\"])\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# MAGIC %md\n",
        "# MAGIC We can also check how many tokens OpenAI has used\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "gpt35_response[\"usage\"][\"total_tokens\"]\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# MAGIC %md\n",
        "# MAGIC The results are noticeably much better compared to when using Hugging Face's GPT-2! It didn't get stuck in the text generation, but the sessions recommended are not all relevant to pandas either. You can further do more prompt engineering to get better results.\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# MAGIC %md ## Submit your Results (edX Verified Only)\n",
        "# MAGIC\n",
        "# MAGIC To get credit for this lab, click the submit button in the top right to report the results. If you run into any issues, click `Run` -> `Clear state and run all`, and make sure all tests have passed before re-submitting. If you accidentally deleted any tests, take a look at the notebook's version history to recover them or reload the notebooks.\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# MAGIC %md-sandbox\n",
        "# MAGIC &copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
        "# MAGIC Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
        "# MAGIC <br/>\n",
        "# MAGIC <a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}